{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Name: Vijaykumar Maraviya\n",
    "\n",
    "Student Number: 1006040320"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What is the purpose of OpenAI gyms and how is it going to help us in our RL education?\n",
    "\n",
    "According to  OpenAI gym documentation, the main objectives of gym is as follows:\n",
    "\n",
    "1) Provide standard set of environments that are easy to setup and use. \n",
    "\n",
    "2) The goal is to have better benchmarks to compare the outcome of research through standardization of problem defination such as reward function or the set of actions.\n",
    "\n",
    "In education, it will let researchers and students focus on learning and development of RL algorithms, rather than spending time on problem defination and setup. It will also make dissemination of knowledge convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HsGqubiDkQnd"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7iS40R9okStg"
   },
   "outputs": [],
   "source": [
    "gym.envs.register(\n",
    "    id='FrozenLakeNotSlippery-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    "    max_episode_steps=100,\n",
    "    reward_threshold=0.74\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "FVX1AjRWkueO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: [(1.0, 0, 0.0, False)],\n",
       "  1: [(1.0, 4, 0.0, False)],\n",
       "  2: [(1.0, 1, 0.0, False)],\n",
       "  3: [(1.0, 0, 0.0, False)]},\n",
       " 1: {0: [(1.0, 0, 0.0, False)],\n",
       "  1: [(1.0, 5, 0.0, True)],\n",
       "  2: [(1.0, 2, 0.0, False)],\n",
       "  3: [(1.0, 1, 0.0, False)]},\n",
       " 2: {0: [(1.0, 1, 0.0, False)],\n",
       "  1: [(1.0, 6, 0.0, False)],\n",
       "  2: [(1.0, 3, 0.0, False)],\n",
       "  3: [(1.0, 2, 0.0, False)]},\n",
       " 3: {0: [(1.0, 2, 0.0, False)],\n",
       "  1: [(1.0, 7, 0.0, True)],\n",
       "  2: [(1.0, 3, 0.0, False)],\n",
       "  3: [(1.0, 3, 0.0, False)]},\n",
       " 4: {0: [(1.0, 4, 0.0, False)],\n",
       "  1: [(1.0, 8, 0.0, False)],\n",
       "  2: [(1.0, 5, 0.0, True)],\n",
       "  3: [(1.0, 0, 0.0, False)]},\n",
       " 5: {0: [(1.0, 5, 0, True)],\n",
       "  1: [(1.0, 5, 0, True)],\n",
       "  2: [(1.0, 5, 0, True)],\n",
       "  3: [(1.0, 5, 0, True)]},\n",
       " 6: {0: [(1.0, 5, 0.0, True)],\n",
       "  1: [(1.0, 10, 0.0, False)],\n",
       "  2: [(1.0, 7, 0.0, True)],\n",
       "  3: [(1.0, 2, 0.0, False)]},\n",
       " 7: {0: [(1.0, 7, 0, True)],\n",
       "  1: [(1.0, 7, 0, True)],\n",
       "  2: [(1.0, 7, 0, True)],\n",
       "  3: [(1.0, 7, 0, True)]},\n",
       " 8: {0: [(1.0, 8, 0.0, False)],\n",
       "  1: [(1.0, 12, 0.0, True)],\n",
       "  2: [(1.0, 9, 0.0, False)],\n",
       "  3: [(1.0, 4, 0.0, False)]},\n",
       " 9: {0: [(1.0, 8, 0.0, False)],\n",
       "  1: [(1.0, 13, 0.0, False)],\n",
       "  2: [(1.0, 10, 0.0, False)],\n",
       "  3: [(1.0, 5, 0.0, True)]},\n",
       " 10: {0: [(1.0, 9, 0.0, False)],\n",
       "  1: [(1.0, 14, 0.0, False)],\n",
       "  2: [(1.0, 11, 0.0, True)],\n",
       "  3: [(1.0, 6, 0.0, False)]},\n",
       " 11: {0: [(1.0, 11, 0, True)],\n",
       "  1: [(1.0, 11, 0, True)],\n",
       "  2: [(1.0, 11, 0, True)],\n",
       "  3: [(1.0, 11, 0, True)]},\n",
       " 12: {0: [(1.0, 12, 0, True)],\n",
       "  1: [(1.0, 12, 0, True)],\n",
       "  2: [(1.0, 12, 0, True)],\n",
       "  3: [(1.0, 12, 0, True)]},\n",
       " 13: {0: [(1.0, 12, 0.0, True)],\n",
       "  1: [(1.0, 13, 0.0, False)],\n",
       "  2: [(1.0, 14, 0.0, False)],\n",
       "  3: [(1.0, 9, 0.0, False)]},\n",
       " 14: {0: [(1.0, 13, 0.0, False)],\n",
       "  1: [(1.0, 14, 0.0, False)],\n",
       "  2: [(1.0, 15, 1.0, True)],\n",
       "  3: [(1.0, 10, 0.0, False)]},\n",
       " 15: {0: [(1.0, 15, 0, True)],\n",
       "  1: [(1.0, 15, 0, True)],\n",
       "  2: [(1.0, 15, 0, True)],\n",
       "  3: [(1.0, 15, 0, True)]}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the gridworld-like environment\n",
    "env=gym.make('FrozenLakeNotSlippery-v0')\n",
    "# Let's look at the model of the environment (i.e., P):\n",
    "env.env.P\n",
    "# presentation of P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_6Sp3Hltz2D"
   },
   "source": [
    "### Question: what is the data in this structure saying? Relate this to the course\n",
    "\n",
    "P contains the dictionary where each key is a state and value is a dictionary. The keys of this inner dictionary are possible actions for that state. Further, each action stores a list of tuples. The first element of the tuple is transition probability, second element is a next state after transition, third element is a reward, and the fourth (last) element is True/False value to indicate if agent is done (reached terminal state). The P describes the environment model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Gyn_w3ulkyZI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(16)\n"
     ]
    }
   ],
   "source": [
    "# Now let's investigate the observation space (i.e., S using our nomenclature),\n",
    "# and confirm we see it is a discrete space with 16 locations\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zND5ArI8k_qQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "stateSpaceSize = env.observation_space.n\n",
    "print(stateSpaceSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "R_tp9YzRljnj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "# Now let's investigate the action space (i.e., A) for the agent->environment\n",
    "# channel\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "fFGNZNowluz2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample from S: 10  ...  sample from A: 1\n",
      "sample from S: 5  ...  sample from A: 2\n",
      "sample from S: 9  ...  sample from A: 0\n",
      "sample from S: 11  ...  sample from A: 1\n",
      "sample from S: 9  ...  sample from A: 3\n",
      "sample from S: 7  ...  sample from A: 2\n",
      "sample from S: 8  ...  sample from A: 3\n",
      "sample from S: 13  ...  sample from A: 0\n",
      "sample from S: 1  ...  sample from A: 2\n"
     ]
    }
   ],
   "source": [
    "# The gym environment has ...sample() functions that allow us to sample\n",
    "# from the above spaces:\n",
    "for g in range(1,10,1):\n",
    "  print(\"sample from S:\",env.observation_space.sample(),\" ... \",\"sample from A:\",env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nOQL5JxsmcEd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# The enviroment also provides a helper to render (visualize) the environment\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "KLV6e43mmwx1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Enter the action as an integer from 0 to 3  (or exit): \n",
      "a\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'a'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-3f1c78a8116e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0muserInput\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m\"exit\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0maction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muserInput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobability\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"--> The result of taking action\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"is:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'a'"
     ]
    }
   ],
   "source": [
    "# We can act as the agent, by selecting actions and stepping the environment\n",
    "# through time to see its responses to our actions\n",
    "env.reset()\n",
    "exitCommand=False\n",
    "while not(exitCommand):\n",
    "    env.render()\n",
    "    print(\"Enter the action as an integer from 0 to\",env.action_space.n -1,\" (or exit): \")\n",
    "    userInput=input()\n",
    "    if userInput==\"exit\":\n",
    "        break\n",
    "    action=int(userInput)\n",
    "    (observation, reward, compute, probability) = env.step(action)\n",
    "    print(\"--> The result of taking action\",action,\"is:\")\n",
    "    print(\"     S=\",observation)\n",
    "    print(\"     R=\",reward)\n",
    "    print(\"     p=\",probability)\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tBpeiuRnyih"
   },
   "source": [
    "### Question: draw a table indicating the correspondence between the action you input (a number) and the logic action performed.\n",
    "\n",
    "| action | logic\n",
    "| --- | ---\n",
    "| 0 | Move Left\n",
    "| 1 | Move Down\n",
    "| 2 | Move Right\n",
    "| 3 | Move Up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tBpeiuRnyih"
   },
   "source": [
    "### Question: draw a table that illustrates what the symbols on the render image mean?\n",
    "\n",
    "| symbol | Meaning\n",
    "| --- | ---\n",
    "| S | Start (Safe)\n",
    "| F | Frozen Surface (Safe)\n",
    "| H | Hole (fall to doom)\n",
    "| G | Goal \n",
    "\n",
    "Episode ends when agent reaches the goal or fall in a Hole. It receives the reward of 1 for reaching the goal and 0 otherwise.\n",
    "\n",
    "\n",
    "### Question: Explain what the objective of the agent is in this environment?\n",
    "\n",
    "The objective of the agent is to reach to a goal tile in a minimum number of steps, while avoiding the holes (that lead to falling in water)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWI3h6s7qqdq"
   },
   "source": [
    "### Practical: Code up an AI that will employ random action selection in order to drive the agent. Test this random action selection agent with the above environment (i.e., code up a loop as I did above, but instead of taking input from a human user, take it from the AI you coded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "YmRwGwPoqw0F"
   },
   "outputs": [],
   "source": [
    "# AI player with random action selection\n",
    "class randomAI:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def select_action(self):\n",
    "        return np.random.choice(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 1, moved to state S=4, and received reward 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 3, moved to state S=0, and received reward 0.0\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 3, moved to state S=0, and received reward 0.0\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 1, moved to state S=4, and received reward 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 1, moved to state S=8, and received reward 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "--> The agent took action 2, moved to state S=9, and received reward 0.0\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=8, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "--> The agent took action 3, moved to state S=4, and received reward 0.0\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 1, moved to state S=8, and received reward 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=8, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=8, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "--> The agent took action 1, moved to state S=12, and received reward 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "\u001b[41mH\u001b[0mFFG\n",
      "Episode finished after 12 timesteps\n"
     ]
    }
   ],
   "source": [
    "# random action selection agent\n",
    "agent = randomAI()\n",
    "env.reset()\n",
    "done=False\n",
    "t = 0\n",
    "while not(done):\n",
    "    env.render()\n",
    "    action = agent.select_action()\n",
    "    (observation, reward, done, probability) = env.step(action)\n",
    "    print(f\"--> The agent took action {action}, moved to state S={observation}, and received reward {reward}\")\n",
    "    t = t + 1\n",
    "    if done:\n",
    "        env.render()\n",
    "        print(f\"Episode finished after {t} timesteps\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQLKuubzrl4L"
   },
   "source": [
    "## Now towards dynamic programming. Note that env.env.P has the model of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQLKuubzrl4L"
   },
   "source": [
    "### Question: How would you represent the agent's policy function and value function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would represent the agent's policy and value functions using dictionaries (tables). The policy function (dictionary) will map state (key) to action (value). The value function (dictionary) will map state (key) to real number (value) representing the lucrativeness of being in that state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQLKuubzrl4L"
   },
   "source": [
    "### Practical: revise the above AI solver to use a policy function in which you code the random action selections in the policy function. Test this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(env):\n",
    "    policy = dict()\n",
    "    for s in range(env.nS):\n",
    "        policy[s] = np.random.choice(env.nA)\n",
    "    return policy\n",
    "\n",
    "# AI agent: action selection using random policy\n",
    "class randomAI:\n",
    "    def __init__(self, env):\n",
    "        self.policy = random_policy(env)\n",
    "        \n",
    "    def act(self, observation):\n",
    "        return self.policy[observation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 1, moved to state S=4, and received reward 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Episode finished after 100 timesteps\n"
     ]
    }
   ],
   "source": [
    "agent = randomAI(env)\n",
    "observation = env.reset()\n",
    "done=False\n",
    "t = 0\n",
    "while not(done):\n",
    "    env.render()\n",
    "    action = agent.act(observation)\n",
    "    (observation, reward, done, probability) = env.step(action)\n",
    "    print(f\"--> The agent took action {action}, moved to state S={observation}, and received reward {reward}\")\n",
    "    t = t + 1\n",
    "    if done:\n",
    "        env.render()\n",
    "        print(f\"Episode finished after {t} timesteps\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQLKuubzrl4L"
   },
   "source": [
    "### Practical: Code the C-4 Policy Evaluation (Prediction) algorithm. You may use either the inplace or ping-pong buffer (as described in the lecture). Now randomly initialize your policy function, and compute its value function. Report your results: policy and value function. Ensure your prediction algo reports how many iterations it took."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(env):\n",
    "    policy = dict()\n",
    "    for s in range(env.nS):\n",
    "        policy[s] = np.random.choice(env.nA)\n",
    "    return policy\n",
    "\n",
    "def evaluate_policy(env, policy):\n",
    "    # threshold for convergence\n",
    "    theta = 1e-6\n",
    "    \n",
    "    # initialize the values to 0 for all states\n",
    "    V = dict(zip(range(env.nS), [0]*env.nS))\n",
    "    \n",
    "    # iteration counter\n",
    "    i = 0\n",
    "    \n",
    "    # interate until convergence\n",
    "    while True:\n",
    "        # update iteration count\n",
    "        i = i+1\n",
    "        \n",
    "        delta = 0\n",
    "        \n",
    "        # loop for each s\n",
    "        for s in range(env.nS):\n",
    "            # old value of s\n",
    "            old_val = V[s]\n",
    "            \n",
    "            # select action based on current policy\n",
    "            a = policy[s]\n",
    "\n",
    "            # update value of s\n",
    "            value = 0\n",
    "            for p, next_s, r, _ in env.P[s][a]:\n",
    "                value += p*(r + 0.9*V[next_s])\n",
    "            V[s] = value \n",
    "            \n",
    "            delta = max(delta, abs(V[s] - old_val))  \n",
    "\n",
    "        # check convergence\n",
    "        if delta < theta:\n",
    "            break\n",
    "            \n",
    "    return V, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly initialized policy: {0: 1, 1: 2, 2: 1, 3: 0, 4: 1, 5: 0, 6: 1, 7: 0, 8: 2, 9: 1, 10: 1, 11: 0, 12: 0, 13: 2, 14: 2, 15: 0}\n",
      "Value function for randomly initialized policy function: {0: 0.5904900000000002, 1: 0.6561000000000001, 2: 0.7290000000000001, 3: 0.6561000000000001, 4: 0.6561000000000001, 5: 0.0, 6: 0.81, 7: 0.0, 8: 0.7290000000000001, 9: 0.81, 10: 0.9, 11: 0.0, 12: 0.0, 13: 0.9, 14: 1.0, 15: 0.0}\n",
      "Number of interation taken policy evaluation: 7\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "policy = random_policy(env)\n",
    "V, i = evaluate_policy(env, policy)\n",
    "print(\"Randomly initialized policy:\", policy)\n",
    "print(\"Value function for randomly initialized policy function:\", V)\n",
    "print(\"Number of interation taken policy evaluation:\", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQLKuubzrl4L"
   },
   "source": [
    "### (Optional): Repeat the above for q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(env):\n",
    "    policy = dict()\n",
    "    for s in range(env.nS):\n",
    "        policy[s] = np.random.choice(env.nA)\n",
    "    return policy\n",
    "\n",
    "def evaluate_policy(env, policy):\n",
    "    # threshold for convergence\n",
    "    theta = 1e-6\n",
    "    \n",
    "    # initialize the values to 0 for all state, action pair\n",
    "    Q = dict(zip([(s,a) for s in range(env.nS) for a in range(env.nA)]\n",
    "                 , [0 for s in range(env.nS) for a in range(env.nA)]))\n",
    "    \n",
    "    # iteration counter\n",
    "    i = 0\n",
    "    \n",
    "    # interate until convergence\n",
    "    while True:\n",
    "        # update iteration count\n",
    "        i = i+1\n",
    "        \n",
    "        delta = 0\n",
    "        \n",
    "        # loop for each s,a pair\n",
    "        for s,a in Q.keys():\n",
    "            \n",
    "            # old value of s,a pair\n",
    "            old_val = Q[(s,a)]\n",
    "\n",
    "            # update value of Q(s,a)\n",
    "            value = 0\n",
    "            for p, next_s, r, _ in env.P[s][a]:\n",
    "                # select action based on current policy Pi(a'|s')\n",
    "                a_ = policy[next_s]\n",
    "                value += p*(r + 0.9*Q[(next_s,a_)])\n",
    "                    \n",
    "            Q[(s,a)] = value \n",
    "            \n",
    "            delta = max(delta, abs(Q[(s,a)] - old_val))  \n",
    "\n",
    "        # check convergence\n",
    "        if delta < theta:\n",
    "            break\n",
    "            \n",
    "    return Q, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly initialized policy: {0: 0, 1: 0, 2: 3, 3: 2, 4: 3, 5: 0, 6: 3, 7: 0, 8: 3, 9: 3, 10: 2, 11: 2, 12: 1, 13: 1, 14: 2, 15: 2} \n",
      "\n",
      "action-Value function for randomly initialized policy function: {(0, 0): 0.0, (0, 1): 0.0, (0, 2): 0.0, (0, 3): 0.0, (1, 0): 0.0, (1, 1): 0.0, (1, 2): 0.0, (1, 3): 0.0, (2, 0): 0.0, (2, 1): 0.0, (2, 2): 0.0, (2, 3): 0.0, (3, 0): 0.0, (3, 1): 0.0, (3, 2): 0.0, (3, 3): 0.0, (4, 0): 0.0, (4, 1): 0.0, (4, 2): 0.0, (4, 3): 0.0, (5, 0): 0.0, (5, 1): 0.0, (5, 2): 0.0, (5, 3): 0.0, (6, 0): 0.0, (6, 1): 0.0, (6, 2): 0.0, (6, 3): 0.0, (7, 0): 0.0, (7, 1): 0.0, (7, 2): 0.0, (7, 3): 0.0, (8, 0): 0.0, (8, 1): 0.0, (8, 2): 0.0, (8, 3): 0.0, (9, 0): 0.0, (9, 1): 0.0, (9, 2): 0.0, (9, 3): 0.0, (10, 0): 0.0, (10, 1): 0.9, (10, 2): 0.0, (10, 3): 0.0, (11, 0): 0.0, (11, 1): 0.0, (11, 2): 0.0, (11, 3): 0.0, (12, 0): 0.0, (12, 1): 0.0, (12, 2): 0.0, (12, 3): 0.0, (13, 0): 0.0, (13, 1): 0.0, (13, 2): 0.9, (13, 3): 0.0, (14, 0): 0.0, (14, 1): 0.9, (14, 2): 1.0, (14, 3): 0.0, (15, 0): 0.0, (15, 1): 0.0, (15, 2): 0.0, (15, 3): 0.0} \n",
      "\n",
      "Number of interation taken for policy evaluation: 3\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "policy = random_policy(env)\n",
    "Q, i = evaluate_policy(env, policy)\n",
    "print(\"Randomly initialized policy:\", policy, '\\n')\n",
    "print(\"action-Value function for randomly initialized policy function:\", Q, '\\n')\n",
    "print(\"Number of interation taken for policy evaluation:\", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQLKuubzrl4L"
   },
   "source": [
    "## Policy Improvement:\n",
    "### Question: How would you use P and your value function to improve an arbitrary policy, pi, per Chapter 4?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would use policy improvement theorem iteratively to improve initial random policy to finally obtain optimal policy.\n",
    "\n",
    "If the value of following the different action 'a' in state 's' and then following our policy for all subsequent states is greater than the value of state s in the current policy, then it is always better to choose action 'a' in state 's'. (Policy Improvement Theorem). If we consider such changes at all states,selecting an action that give the higer state value for each state, we obtain the greedy policy for a given state-value function. This is called policy improvement.\n",
    "\n",
    "Then we evaluate new greedy policy and obtain a new state-value function. Policy improvement must give strictly better state-value function except when the original policy is already optimal. We use this as a condition for termination.\n",
    "\n",
    "In summary, we use environment model P and the value function for a current policy to obtain a new greedy policy. Then we obtain the value function for new greedy policy through policy evaluation. We continue the loop of policy evaulation and policy improvement until we converge to optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQLKuubzrl4L"
   },
   "source": [
    "### Practical: Code the policy iteration process, and employ it to arrive at a policy that solves this problem. Show your testing results, and ensure it reports the number of iterations for each step: (a) overall policy iteration steps and (b) evaluation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(env):\n",
    "    policy = dict()\n",
    "    for s in range(env.nS):\n",
    "        policy[s] = np.random.choice(env.nA)\n",
    "    return policy\n",
    "\n",
    "def evaluate_policy(env, policy):\n",
    "    # threshold for convergence\n",
    "    theta = 1e-6\n",
    "    \n",
    "    # initialize the values to 0 for all states\n",
    "    V = dict(zip(range(env.nS), [0]*env.nS))\n",
    "    \n",
    "    # iteration counter\n",
    "    i = 0\n",
    "    \n",
    "    # interate until convergence\n",
    "    while True:\n",
    "        # update iteration count\n",
    "        i = i+1\n",
    "        \n",
    "        delta = 0\n",
    "        \n",
    "        # loop for each s\n",
    "        for s in range(env.nS):\n",
    "            # old value of s\n",
    "            old_val = V[s]\n",
    "            \n",
    "            # select action based on current policy\n",
    "            a = policy[s]\n",
    "\n",
    "            # update value of s\n",
    "            value = 0\n",
    "            for p, next_s, r, _ in env.P[s][a]:\n",
    "                value += p*(r + 0.9*V[next_s])\n",
    "            V[s] = value \n",
    "            \n",
    "            delta = max(delta, abs(V[s] - old_val))  \n",
    "\n",
    "        # check convergence\n",
    "        if delta < theta:\n",
    "            break\n",
    "            \n",
    "    return V, i\n",
    "\n",
    "def greedy_policy(env, V):\n",
    "    policy = dict()\n",
    "    \n",
    "    # loop for each s\n",
    "    for s in range(env.nS):\n",
    "        a_vals = np.zeros(env.nA)\n",
    "        \n",
    "        # obtain value for each possible action a\n",
    "        for a in range(env.nA):\n",
    "            value = 0\n",
    "            for p, next_s, r, _ in env.P[s][a]:\n",
    "                value += p*(r + 0.9*V[next_s])\n",
    "            a_vals[a] = value\n",
    "            \n",
    "        # select action with maximum value\n",
    "        policy[s] = np.argmax(a_vals)\n",
    "        \n",
    "    return policy\n",
    "\n",
    "def policy_iteration(env):\n",
    "    # initialize random policy\n",
    "    old_policy = random_policy(env)\n",
    "    \n",
    "    # track number of iterations\n",
    "    n_policy_itr = 0\n",
    "    n_policy_eval = 0\n",
    "    \n",
    "    # iterate until convergence\n",
    "    while True:\n",
    "        n_policy_itr += 1\n",
    "        \n",
    "        #evaluate policy\n",
    "        V, i = evaluate_policy(env, old_policy)\n",
    "        n_policy_eval += i\n",
    "        # update policy\n",
    "        new_policy = greedy_policy(env, V)\n",
    "        \n",
    "        #check convergence\n",
    "        if old_policy == new_policy:\n",
    "            break\n",
    "            \n",
    "        old_policy = new_policy\n",
    "        \n",
    "    print(f\"The total number of policy evaluation steps: {n_policy_eval}\")\n",
    "    print(f\"The total number of policy iteration steps: {n_policy_itr}\")\n",
    "    \n",
    "    return new_policy, V\n",
    "\n",
    "# AI agent: action selection using optimum policy\n",
    "class AIPlayer:\n",
    "    def __init__(self, env):\n",
    "        self.policy, self.V = policy_iteration(env)\n",
    "        self.env = env\n",
    "        \n",
    "    def act(self, observation):\n",
    "        return self.policy[observation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of policy evaluation steps: 28\n",
      "The total number of policy iteration steps: 7\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "player = AIPlayer(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimum policy: {0: 1, 1: 2, 2: 1, 3: 0, 4: 1, 5: 0, 6: 1, 7: 0, 8: 2, 9: 1, 10: 1, 11: 0, 12: 0, 13: 2, 14: 2, 15: 0}\n",
      "optimum value function: {0: 0.5904900000000002, 1: 0.6561000000000001, 2: 0.7290000000000001, 3: 0.6561000000000001, 4: 0.6561000000000001, 5: 0.0, 6: 0.81, 7: 0.0, 8: 0.7290000000000001, 9: 0.81, 10: 0.9, 11: 0.0, 12: 0.0, 13: 0.9, 14: 1.0, 15: 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"optimum policy:\", player.policy)\n",
    "print(\"optimum value function:\", player.V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test results (policy iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 1, moved to state S=4, and received reward 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 1, moved to state S=8, and received reward 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "--> The agent took action 2, moved to state S=9, and received reward 0.0\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "--> The agent took action 1, moved to state S=13, and received reward 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "--> The agent took action 2, moved to state S=14, and received reward 0.0\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "--> The agent took action 2, moved to state S=15, and received reward 1.0\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Episode finished after 6 timesteps\n"
     ]
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "done=False\n",
    "t = 0\n",
    "while not(done):\n",
    "    env.render()\n",
    "    action = player.act(observation)\n",
    "    (observation, reward, done, probability) = env.step(action)\n",
    "    print(f\"--> The agent took action {action}, moved to state S={observation}, and received reward {reward}\")\n",
    "    t = t + 1\n",
    "    if done:\n",
    "        env.render()\n",
    "        print(f\"Episode finished after {t} timesteps\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQLKuubzrl4L"
   },
   "source": [
    "### Practical: Code the value iteration process, and employ it to arrive at a policy that solves this problem. Show your testing results, reporting the iteration counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(env, V):\n",
    "    policy = dict()\n",
    "    \n",
    "    # loop for each s\n",
    "    for s in range(env.nS):\n",
    "        a_vals = np.zeros(env.nA)\n",
    "        \n",
    "        # obtain value for each possible action a\n",
    "        for a in range(env.nA):\n",
    "            value = 0\n",
    "            for p, next_s, r, _ in env.P[s][a]:\n",
    "                value += p*(r + 0.9*V[next_s])\n",
    "            a_vals[a] = value\n",
    "            \n",
    "        # select action with maximum value\n",
    "        policy[s] = np.argmax(a_vals)\n",
    "        \n",
    "    return policy\n",
    "\n",
    "def value_iteration(env):\n",
    "    # threshold for convergence\n",
    "    theta = 1e-6\n",
    "    \n",
    "    # initialize the values to 0 for all states\n",
    "    V = dict(zip(range(env.nS), [0]*env.nS))\n",
    "    \n",
    "    # iteration counter\n",
    "    i = 0\n",
    "    \n",
    "    # interate until convergence\n",
    "    while True:\n",
    "        # update iteration count\n",
    "        i = i+1\n",
    "        \n",
    "        delta = 0\n",
    "        \n",
    "        # loop for each s\n",
    "        for s in range(env.nS):\n",
    "            # old value of s\n",
    "            old_val = V[s]\n",
    "            \n",
    "            # obtain value for each possible action a\n",
    "            a_vals = np.zeros(env.nA)\n",
    "            for a in range(env.nA):\n",
    "                value = 0\n",
    "                for p, next_s, r, _ in env.P[s][a]:\n",
    "                    value += p*(r + 0.9*V[next_s])\n",
    "                a_vals[a] = value\n",
    "\n",
    "            # update value of s               \n",
    "            V[s] = np.max(a_vals)\n",
    "            \n",
    "            delta = max(delta, abs(V[s] - old_val))  \n",
    "\n",
    "        # check convergence\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    print(f\"The total number of value iteration steps: {i}\")\n",
    "            \n",
    "    policy = greedy_policy(env, V)\n",
    "            \n",
    "    return policy, V\n",
    "\n",
    "# AI agent: action selection using optimum policy\n",
    "class AIPlayer:\n",
    "    def __init__(self, env):\n",
    "        self.policy, self.V = value_iteration(env)\n",
    "        self.env = env\n",
    "        \n",
    "    def act(self, observation):\n",
    "        return self.policy[observation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of value iteration steps: 7\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "player = AIPlayer(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimum policy: {0: 1, 1: 2, 2: 1, 3: 0, 4: 1, 5: 0, 6: 1, 7: 0, 8: 2, 9: 1, 10: 1, 11: 0, 12: 0, 13: 2, 14: 2, 15: 0}\n",
      "optimum value function: {0: 0.5904900000000002, 1: 0.6561000000000001, 2: 0.7290000000000001, 3: 0.6561000000000001, 4: 0.6561000000000001, 5: 0.0, 6: 0.81, 7: 0.0, 8: 0.7290000000000001, 9: 0.81, 10: 0.9, 11: 0.0, 12: 0.0, 13: 0.9, 14: 1.0, 15: 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"optimum policy:\", player.policy)\n",
    "print(\"optimum value function:\", player.V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test results (value iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 1, moved to state S=4, and received reward 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 1, moved to state S=8, and received reward 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "--> The agent took action 2, moved to state S=9, and received reward 0.0\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "--> The agent took action 1, moved to state S=13, and received reward 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "--> The agent took action 2, moved to state S=14, and received reward 0.0\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "--> The agent took action 2, moved to state S=15, and received reward 1.0\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Episode finished after 6 timesteps\n"
     ]
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "done=False\n",
    "t = 0\n",
    "while not(done):\n",
    "    env.render()\n",
    "    action = player.act(observation)\n",
    "    (observation, reward, done, probability) = env.step(action)\n",
    "    print(f\"--> The agent took action {action}, moved to state S={observation}, and received reward {reward}\")\n",
    "    t = t + 1\n",
    "    if done:\n",
    "        env.render()\n",
    "        print(f\"Episode finished after {t} timesteps\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment on the difference between the iterations required for policy vs value iteration.\n",
    "\n",
    "The policy iteration required in total 35 iterations: 7 policy improvement iterations and 28 policy evaluation iterations. The value iteration only required 7 evaluation iterations (to obtain optimum state value function) and 1 policy improvement iteration (for greedy policy selection).\n",
    "\n",
    "Effectively, value iteration in each of its sweep through states combine one sweep of policy evaluation and one sweep of policy improvement. Whereas each sweep of policy improvement required 7 sweeps through states for policy iteration. \n",
    "\n",
    "In general, if state space is large, value iteration would converge significantly faster than policy iteration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQLKuubzrl4L"
   },
   "source": [
    "### Optional: instead of the above environment, use the \"slippery\" Frozen Lake via env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(env, V):\n",
    "    policy = dict()\n",
    "    \n",
    "    # loop for each s\n",
    "    for s in range(env.nS):\n",
    "        a_vals = np.zeros(env.nA)\n",
    "        \n",
    "        # obtain value for each possible action a\n",
    "        for a in range(env.nA):\n",
    "            value = 0\n",
    "            for p, next_s, r, _ in env.P[s][a]:\n",
    "                value += p*(r + 0.9*V[next_s])\n",
    "            a_vals[a] = value\n",
    "            \n",
    "        # select action with maximum value\n",
    "        policy[s] = np.argmax(a_vals)\n",
    "        \n",
    "    return policy\n",
    "\n",
    "def value_iteration(env):\n",
    "    # threshold for convergence\n",
    "    theta = 1e-6\n",
    "    \n",
    "    # initialize the values to 0 for all states\n",
    "    V = dict(zip(range(env.nS), [0]*env.nS))\n",
    "    \n",
    "    # iteration counter\n",
    "    i = 0\n",
    "    \n",
    "    # interate until convergence\n",
    "    while True:\n",
    "        # update iteration count\n",
    "        i = i+1\n",
    "        \n",
    "        delta = 0\n",
    "        \n",
    "        # loop for each s\n",
    "        for s in range(env.nS):\n",
    "            # old value of s\n",
    "            old_val = V[s]\n",
    "            \n",
    "            # obtain value for each possible action a\n",
    "            a_vals = np.zeros(env.nA)\n",
    "            for a in range(env.nA):\n",
    "                value = 0\n",
    "                for p, next_s, r, _ in env.P[s][a]:\n",
    "                    value += p*(r + 0.9*V[next_s])\n",
    "                a_vals[a] = value\n",
    "\n",
    "            # update value of s               \n",
    "            V[s] = np.max(a_vals)\n",
    "            \n",
    "            delta = max(delta, abs(V[s] - old_val))  \n",
    "\n",
    "        # check convergence\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    print(f\"The total number of value iteration steps: {i}\")\n",
    "            \n",
    "    policy = greedy_policy(env, V)\n",
    "            \n",
    "    return policy, V\n",
    "\n",
    "# AI agent: action selection using optimum policy\n",
    "class AIPlayer:\n",
    "    def __init__(self, env):\n",
    "        self.policy, self.V = value_iteration(env)\n",
    "        self.env = env\n",
    "        \n",
    "    def act(self, observation):\n",
    "        return self.policy[observation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slippery environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of value iteration steps: 60\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "player = AIPlayer(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=0, and received reward 0.0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=4, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The agent took action 0, moved to state S=8, and received reward 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "--> The agent took action 3, moved to state S=9, and received reward 0.0\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "--> The agent took action 1, moved to state S=13, and received reward 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "--> The agent took action 2, moved to state S=9, and received reward 0.0\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "--> The agent took action 1, moved to state S=13, and received reward 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "--> The agent took action 2, moved to state S=14, and received reward 0.0\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "--> The agent took action 1, moved to state S=14, and received reward 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "--> The agent took action 1, moved to state S=15, and received reward 1.0\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Episode finished after 11 timesteps\n"
     ]
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "done=False\n",
    "t = 0\n",
    "while not(done):\n",
    "    env.render()\n",
    "    action = player.act(observation)\n",
    "    (observation, reward, done, probability) = env.step(action)\n",
    "    print(f\"--> The agent took action {action}, moved to state S={observation}, and received reward {reward}\")\n",
    "    t = t + 1\n",
    "    if done:\n",
    "        env.render()\n",
    "        print(f\"Episode finished after {t} timesteps\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_reward: 0.732100\n"
     ]
    }
   ],
   "source": [
    "av_reward = []\n",
    "for i_episode in range(10000):\n",
    "    observation = env.reset()\n",
    "    for t in range(10000):\n",
    "        action = player.act(observation)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "    av_reward.append(reward)\n",
    "print('avg_reward: %f' % np.mean(av_reward))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "A1_FrozenLake.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "MIE1624_env",
   "language": "python",
   "name": "mie1624_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
