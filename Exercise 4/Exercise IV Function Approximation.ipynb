{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use this exercise as an opportunity to revise the concepts of pytorch. So before using the modules and classes such as `torch.nn`, `torch.optim`, `Dataset`, `DataLoader` to create and train a neural network, I will build a simple network from scratch and then replace each component with the functionality provided by the mentioned modules, gradually. For this, I am using the tutorial provided in the pytorch documentation here: https://pytorch.org/tutorials/beginner/nn_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset setup\n",
    "\n",
    "I use the MNIST dataset, which consist of images of digits (between 0 to 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "# setup the path to store the data\n",
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "# create the directory \n",
    "PATH.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "URL = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "# if file not exist, download\n",
    "if not (PATH / FILENAME).exists():\n",
    "        content = requests.get(URL + FILENAME).content\n",
    "        (PATH / FILENAME).open(\"wb\").write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is stored in numpy arrays and pickled. The following block of code will read the file and store it in a varaible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_train contains 50,000 samples. each sample is a vector of length 784."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x237daf116d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.imshow(x_train[0]. reshape((28,28)), cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch is a replacement to numpy to use the power of a gpu. It uses `torch.tensor` instead of `numpy.array`. So we will convert the data into tensor format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])\n",
      "torch.Size([50000, 784])\n",
      "tensor(0) tensor(9)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# map numpy array to torch.tensor\n",
    "x_train, y_train, x_valid, y_valid = map(\n",
    "    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
    ")\n",
    "# shape of training data\n",
    "n, c = x_train.shape\n",
    "\n",
    "print(x_train, y_train)\n",
    "print(x_train.shape)\n",
    "print(y_train.min(), y_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create two tensors for weights and bias. The argument requires_grad = True causes pytorch to record all the oprations doen on the tensor so that it can calculate the gradient during backprop automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# similar to numpy, generates 2D tensor\n",
    "weights = torch.randn(784,10)/ math.sqrt(784)\n",
    "# set requires gradient true\n",
    "weights.requires_grad_()\n",
    "# we set requires gradient true while creating the tensor\n",
    "bias = torch.zeros(10, requires_grad = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our model and activation function. pytorch provides modules for this which will be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
    "\n",
    "# 1 layer network with log_softmax activation\n",
    "def model(xb):\n",
    "    return log_softmax(xb @ weights + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.4306, -2.3667, -1.5934, -2.4719, -2.8305, -2.6258, -2.7941, -2.0013,\n",
      "        -2.2031, -2.3814], grad_fn=<SelectBackward>) torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "# batch size\n",
    "bs = 64 \n",
    "\n",
    "# example of an forward pass\n",
    "# minibatch from train set\n",
    "xb = x_train[0:bs]\n",
    "# apply our function to batch\n",
    "preds = model(xb)\n",
    "\n",
    "print(preds[0], preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the output is a vector of length 10. Currently, the model is not trained (randomly intialized weights and bias), hence, the output is a gibberish. \n",
    "\n",
    "We will use neagtive log-liklihood as a loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(input, target):\n",
    "    return -input[range(target.shape[0]), target].mean()\n",
    "\n",
    "loss_func = nll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check the loss of our model on a minibatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4351, grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "# true value\n",
    "yb = y_train[0:bs]\n",
    "print(loss_func(preds, yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: if a index with the largest value in the output vector matches the target value, the the prediction is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0625)\n"
     ]
    }
   ],
   "source": [
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()\n",
    "\n",
    "print(accuracy(preds, yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process:\n",
    "    for each iteration\n",
    "    - select a mini-batch of data\n",
    "    - use model to make predictions\n",
    "    - calculate the loss\n",
    "    - `loss.backward()` updates the gradient of the model (weights and bias)\n",
    "   \n",
    "`loss.backward()` adds the gradient to whatever is already stored. So after each batch, we manually set the gradient to zero before the next loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate\n",
    "lr = 0.5\n",
    "\n",
    "epochs = 2 # number of pass through data\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # for each batch (total 782 batches size 64)\n",
    "    for i in range ((n-1)//bs+1):\n",
    "        # batch start and end index\n",
    "        start_i = i*bs\n",
    "        end_i = start_i +bs\n",
    "        # current batch\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        # make prediction\n",
    "        pred = model(xb)\n",
    "        # compute loss\n",
    "        loss = loss_func(pred, yb)\n",
    "        \n",
    "        # compute gradient\n",
    "        loss.backward()\n",
    "        \n",
    "        # update params and set the gradient to zero\n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * lr\n",
    "            bias -= bias.grad * lr\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all. We have created a minimal neural network (actually logistic regression as there is only 1 layer). \n",
    "Next, let's compare the loss before and after training. We expect the loss to decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0836, grad_fn=<NegBackward>) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use torch modules and library for more cleaner and concise code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn.functional\n",
    "\n",
    "We can replace our hand written activation and loss function with those from the nn.functional module. This module contains wide variety of loss and activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn.Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We subclass nn.Module to create a class that holds weights, bias, and a method for forward pass. nn.Module provides a number of attributes and methods useful for downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(784,10)/math.sqrt(784))\n",
    "        self.bias = nn.Parameter(torch.zeros(10))\n",
    "        \n",
    "    def forward(self,xb):\n",
    "        return xb @ self.weights + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate the object of our class. The object is callable and executes forward method when called. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3992, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = Mnist_Logistic()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we had to update the parameters manually. Now we use `model.parameters()` and `model.zero_grad()` to make those step concise. Also we add the training code inside a fit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n-1)// bs+1):\n",
    "            start_i = i * bs\n",
    "            end_i = start_i + bs\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= p.grad*lr\n",
    "                model.zero_grad()\n",
    "\n",
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0845, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# check that our loss has gone down\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, rather than defining our parameters and network manually, we will use nn.Linear class, which do that for us. pytorch has many predefined layers to simplify the construction of a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(784, 10)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        return self.lin(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3474, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# instantiate the model and claculate the loss as before\n",
    "model = Mnist_Logistic()\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0817, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "fit()\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our training loop, we manually coded the optimization step, updating one parameter at a time inside the loop. `torch.optim` provides various optimization algorithms such Adam, SGD, etc. We can use .step() method  to update parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0825, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "from torch import optim \n",
    "\n",
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n - 1) // bs + 1):\n",
    "            start_i = i * bs\n",
    "            end_i = start_i + bs\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "# also put model and optimizer in a function\n",
    "def get_model():\n",
    "    model = Mnist_Logistic()\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "model, opt = get_model()\n",
    "fit()\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By defining a length and way of indexing, TensorDataset class gives us a way to iterate, index, and slice along the firat dimension of a tensor. This will make it easy to access dependent and independent varaibles in the same line as we train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0817, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "\n",
    "# now we can iterate through minibatches in a single line\n",
    "# xb, yb = train_ds[i*bs : i*bs +bs]\n",
    "\n",
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        xb, yb = train_ds[i * bs: i * bs + bs]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader manages batches. We crate DataLoader from any Dataset. DataLoader gives us a minibatch automatically, rather than us having to slices the dataset manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0815, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "\n",
    "# data loader\n",
    "train_dl = DataLoader(train_ds, batch_size = bs)\n",
    "\n",
    "# our codes upadate to this\n",
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # we iterate over batches rather than indexing and slicing\n",
    "    for xb, yb in train_dl: \n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now add validation set and assess the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: tensor(0.3171)\n",
      "epoch: 1 loss: tensor(0.2877)\n"
     ]
    }
   ],
   "source": [
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "\n",
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=bs * 2)\n",
    "\n",
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = sum(loss_func(model(xb), yb) for xb, yb in valid_dl)\n",
    "\n",
    "    print(\"epoch:\", epoch, \"loss:\" , valid_loss / len(valid_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do some more refactoring for readability of the code. Since we are calculating loss for both training and validation set, we will create a new function loss_batch that computes a loss for one batch. If optimizer is passed, it performs backprop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit perform the training and reports the validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
    "            )\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "\n",
    "        print(\"epoch:\", epoch, \"loss:\" , valid_loss / len(valid_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_data returns the dataloader for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=bs * 2),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can specify the entire process in 3 lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: tensor(0.2877)\n",
      "epoch: 1 loss: tensor(0.2877)\n"
     ]
    }
   ],
   "source": [
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "model, opt = get_model()\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create more advanced NN (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to demonstrate the usage of code, we will create CNN to classify digits using our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: tensor(0.2877)\n",
      "epoch: 1 loss: tensor(0.2877)\n"
     ]
    }
   ],
   "source": [
    "class Mnist_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        xb = xb.view(-1, 1, 28, 28)\n",
    "        xb = F.relu(self.conv1(xb))\n",
    "        xb = F.relu(self.conv2(xb))\n",
    "        xb = F.relu(self.conv3(xb))\n",
    "        xb = F.avg_pool2d(xb, 4)\n",
    "        return xb.view(-1, xb.size(1))\n",
    "\n",
    "lr = 0.1\n",
    "model = Mnist_CNN()\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn.Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential module provides a handy way to create a model. Sequencial object runs each of the modules contained within it in a sequencial manner. We can create a custom layer from a function and use it when defining the network with sequencial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: tensor(0.2877)\n",
      "epoch: 1 loss: tensor(0.2877)\n"
     ]
    }
   ],
   "source": [
    "# define a custome layer\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "\n",
    "\n",
    "def preprocess(x):\n",
    "    return x.view(-1, 1, 28, 28)\n",
    "\n",
    "# define a model with Sequential\n",
    "model = nn.Sequential(\n",
    "    Lambda(preprocess),\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(4),\n",
    "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
    ")\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can use GPU to train faster and use .to(dev) method to send tensor to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the device object\n",
    "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, we use preprocessing layer within our network. Rather, we can write a wrapper to the dataloader to do the preprocessing. Thia way we can abstarct our model to use with different shape of datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: tensor(0.2877)\n",
      "epoch: 1 loss: tensor(0.2877)\n"
     ]
    }
   ],
   "source": [
    "def preprocess(x, y):\n",
    "    return x.view(-1, 1, 28, 28).to(dev), y.to(dev)\n",
    "\n",
    "class WrappedDataLoader:\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "    def __iter__(self):\n",
    "        batches = iter(self.dl)\n",
    "        for b in batches:\n",
    "            yield (self.func(*b))\n",
    "\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "train_dl = WrappedDataLoader(train_dl, preprocess)\n",
    "valid_dl = WrappedDataLoader(valid_dl, preprocess)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
    ")\n",
    "\n",
    "model.to(dev)\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code shown is the final code that we need to create and train neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stochastic_gradient_descent_example from class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data (plotted at the end of this file)\n",
    "train = np.array([[-2,-10], [-1,-5], [0, 0], [1,5], [2,10]])\n",
    "x_train = torch.Tensor(train[:, 0])\n",
    "y_train = torch.Tensor(train[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for stochastic gradient descent, create batches\n",
    "batch_size = 1\n",
    "\n",
    "# dataset and dataloader\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# fit method for training\n",
    "\n",
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl):\n",
    "    for epoch in range(epochs):\n",
    "        # train the model\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "\n",
    "        # evaluate on the train set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in train_dl]\n",
    "            )\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "\n",
    "        print(\"epoch:\", epoch, \"loss:\" , valid_loss / len(valid_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: tensor(0.2877)\n",
      "epoch: 1 loss: tensor(0.2877)\n",
      "epoch: 2 loss: tensor(0.2877)\n",
      "epoch: 3 loss: tensor(0.2877)\n",
      "epoch: 4 loss: tensor(0.2877)\n",
      "epoch: 5 loss: tensor(0.2877)\n",
      "epoch: 6 loss: tensor(0.2877)\n",
      "epoch: 7 loss: tensor(0.2877)\n",
      "epoch: 8 loss: tensor(0.2877)\n",
      "epoch: 9 loss: tensor(0.2877)\n",
      "epoch: 10 loss: tensor(0.2877)\n",
      "epoch: 11 loss: tensor(0.2877)\n",
      "epoch: 12 loss: tensor(0.2877)\n",
      "epoch: 13 loss: tensor(0.2877)\n",
      "epoch: 14 loss: tensor(0.2877)\n",
      "epoch: 15 loss: tensor(0.2877)\n",
      "epoch: 16 loss: tensor(0.2877)\n",
      "epoch: 17 loss: tensor(0.2877)\n",
      "epoch: 18 loss: tensor(0.2877)\n",
      "epoch: 19 loss: tensor(0.2877)\n",
      "epoch: 20 loss: tensor(0.2877)\n",
      "epoch: 21 loss: tensor(0.2877)\n",
      "epoch: 22 loss: tensor(0.2877)\n",
      "epoch: 23 loss: tensor(0.2877)\n",
      "epoch: 24 loss: tensor(0.2877)\n",
      "epoch: 25 loss: tensor(0.2877)\n",
      "epoch: 26 loss: tensor(0.2877)\n",
      "epoch: 27 loss: tensor(0.2877)\n",
      "epoch: 28 loss: tensor(0.2877)\n",
      "epoch: 29 loss: tensor(0.2877)\n",
      "epoch: 30 loss: tensor(0.2877)\n",
      "epoch: 31 loss: tensor(0.2877)\n",
      "epoch: 32 loss: tensor(0.2877)\n",
      "epoch: 33 loss: tensor(0.2877)\n",
      "epoch: 34 loss: tensor(0.2877)\n",
      "epoch: 35 loss: tensor(0.2877)\n",
      "epoch: 36 loss: tensor(0.2877)\n",
      "epoch: 37 loss: tensor(0.2877)\n",
      "epoch: 38 loss: tensor(0.2877)\n",
      "epoch: 39 loss: tensor(0.2877)\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(1,64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64,64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 1)   \n",
    ")\n",
    "\n",
    "alpha = 0.001\n",
    "opt = optim.SGD(model.parameters(), lr=alpha)\n",
    "loss_func = nn.MSELoss()\n",
    "epochs = 40\n",
    "\n",
    "# fit the model\n",
    "fit(epochs, model, loss_func, opt, train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x237cbbae4f0>,\n",
       " <matplotlib.lines.Line2D at 0x237cbbae5e0>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test the model\n",
    "D=np.arange(-2,2,.1).reshape((40,1))\n",
    "R=model(torch.Tensor(D)).detach().numpy()\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(x_train,y_train,'gx',D,R,'r-')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MIE1624_env",
   "language": "python",
   "name": "mie1624_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
