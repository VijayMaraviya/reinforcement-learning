{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from math import inf\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display as ipythondisplay\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from itertools import product\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the device object\n",
    "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mountain Car environment and discretization of state space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_env = gym.make('MountainCar-v0')\n",
    "mc_env.reset()\n",
    "\n",
    "# action_space\n",
    "mc_action_space = [0,1,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function selects an action using e-greedy policy for a given q_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy(q_hat, eps, S, action_space):\n",
    "    # random action with probability eps\n",
    "    if np.random.random() < eps:\n",
    "        return np.random.choice(action_space)\n",
    "    \n",
    "    # greedy action otherwise\n",
    "    act_vals = np.array([q_hat(feature(S,a)).cpu().detach().numpy() for a in action_space])\n",
    "    \n",
    "    return np.random.choice(np.where(act_vals == act_vals.max())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay_eps(current_eps, eps_min, eps_dec):\n",
    "    new_eps = current_eps - eps_dec\n",
    "    return max(new_eps, eps_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(s,a):\n",
    "    np_feature = np.append(s,a)\n",
    "    return torch.from_numpy(np_feature).float().to(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network model to represent the action-value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size_1 = 128, hidden_size_2 = 128):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size_1)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size_1, hidden_size_2)\n",
    "        self.activation2 = nn.ReLU()\n",
    "        self.l3 = nn.Linear(hidden_size_2, 1)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        pred = self.l1(X)\n",
    "        pred = self.activation1(pred)\n",
    "        pred = self.l2(pred)\n",
    "        pred = self.activation2(pred)\n",
    "        pred = self.l3(pred)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_input_size  = mc_env.observation_space.shape[0] + 1\n",
    "mc_q_hat = NeuralNet(mc_input_size, hidden_size_1 = 128, hidden_size_2 = 128)\n",
    "mc_q_hat.to(dev)\n",
    "optimiser = optim.Adam(mc_q_hat.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episodic semi-gradient n-step SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_step_sarsa(env, action_space, q_hat, opt,\n",
    "                 max_episodes = 50000, GAMMA = 1.0,\n",
    "                 EPS_MAX = 1.0, EPS_MIN = 0.05, n=1,\n",
    "                 loss_fn = nn.MSELoss()):\n",
    "    \n",
    "    # set seed for reproducible results\n",
    "    env.seed(0)\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    # epsilon decay per episode\n",
    "    eps_dec = (EPS_MAX - EPS_MIN)*2/max_episodes\n",
    "    eps = EPS_MAX\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for episode in range(max_episodes):\n",
    "        T = inf\n",
    "        t = 0\n",
    "\n",
    "        # storage\n",
    "        states = [0]*(n+1)\n",
    "        actions = [0]*(n+1)\n",
    "        rewards = [0]*(n+1)\n",
    "\n",
    "        # initialize S and store\n",
    "        S = env.reset()\n",
    "        states[t % (n+1)] = S\n",
    "\n",
    "        # choose A and store\n",
    "        A = e_greedy(q_hat, eps, S, action_space)\n",
    "        actions[t % (n+1)] = A\n",
    "\n",
    "        score = 0\n",
    "        while True:\n",
    "            if t < T:\n",
    "                # take action A, observe R and S_next\n",
    "                S, R, done, _ = env.step(A)\n",
    "\n",
    "                score += R\n",
    "\n",
    "                # store R and S_next\n",
    "                rewards[(t+1) % (n+1)] = R\n",
    "                states[(t+1) % (n+1)] = S\n",
    "\n",
    "                if done:\n",
    "                    T = t + 1\n",
    "                else:\n",
    "                    # choose and store A_next\n",
    "                    A = e_greedy(q_hat, eps, S, action_space)\n",
    "                    actions[(t+1) % (n+1)] = A\n",
    "\n",
    "            tau = t - n + 1\n",
    "            if tau >= 0:\n",
    "                G = [GAMMA**(i-tau-1)*rewards[i % (n+1)]\n",
    "                     for i in range(tau+1, min(tau+n, T) + 1)]\n",
    "                G = [np.sum(G)]\n",
    "\n",
    "                if tau + n < T:\n",
    "                    s = states[(tau+n) % (n+1)]\n",
    "                    a = actions[(tau+n) % (n+1)]\n",
    "                    G += (GAMMA**n) * (q_hat(feature(s,a)).cpu().detach().numpy())\n",
    "\n",
    "                G = torch.tensor(G).float().to(dev)\n",
    "\n",
    "                s = states[tau % (n+1)]\n",
    "                a = actions[tau % (n+1)]\n",
    "                # predict the value\n",
    "                pred = q_hat(feature(s,a))\n",
    "                # compute gradient\n",
    "                loss = loss_fn(pred, G)\n",
    "                loss.backward()\n",
    "                # update the params\n",
    "                opt.step()\n",
    "                opt.zero_grad()             \n",
    "                \n",
    "            t += 1\n",
    "            if tau == T - 1:\n",
    "                break\n",
    "        \n",
    "        eps = decay_eps(eps, EPS_MIN, eps_dec)\n",
    "        \n",
    "        scores.append(score)\n",
    "        avg_score = np.mean(scores[-100:])\n",
    "        \n",
    "        if episode % 100 == 0:\n",
    "            print('episode:', episode, '| avg_reward for last 100 episodes: %.1f' % avg_score)\n",
    "            \n",
    "    return q_hat, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 100 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 200 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 300 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 400 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 500 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 600 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 700 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 800 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 900 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 1000 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 1100 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 1200 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 1300 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 1400 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 1500 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 1600 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 1700 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 1800 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 1900 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 2000 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 2100 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 2200 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 2300 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 2400 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 2500 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 2600 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 2700 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 2800 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 2900 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 3000 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 3100 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 3200 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 3300 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 3400 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 3500 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 3600 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 3700 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 3800 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 3900 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 4000 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 4100 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 4200 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 4300 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 4400 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 4500 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 4600 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 4700 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 4800 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 4900 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 5000 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 5100 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 5200 | avg_reward for last 100 episodes: -200.0\n",
      "episode: 5300 | avg_reward for last 100 episodes: -200.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-eec8033b085d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m q_hat_2_step_sarsa, rewards_2_step_sarsa = n_step_sarsa(env = mc_env, \n\u001b[0m\u001b[0;32m      2\u001b[0m                                                         \u001b[0maction_space\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmc_action_space\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                                                         \u001b[0mq_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmc_q_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                                         \u001b[0mmax_episodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                                         n=4)\n",
      "\u001b[1;32m<ipython-input-42-aac8673d0a78>\u001b[0m in \u001b[0;36mn_step_sarsa\u001b[1;34m(env, action_space, q_hat, opt, max_episodes, GAMMA, EPS_MAX, EPS_MIN, n, loss_fn)\u001b[0m\n\u001b[0;32m     59\u001b[0m                     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtau\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m                     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtau\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                     \u001b[0mG\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mGAMMA\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mq_hat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m                 \u001b[0mG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\D\\Desktop\\MIE1624_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-40-9d186f1cd0f2>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\D\\Desktop\\MIE1624_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\D\\Desktop\\MIE1624_env\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\D\\Desktop\\MIE1624_env\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1690\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1692\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1693\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1694\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "q_hat_2_step_sarsa, rewards_2_step_sarsa = n_step_sarsa(env = mc_env, \n",
    "                                                        action_space = mc_action_space,\n",
    "                                                        q_hat = mc_q_hat, opt = optimiser,\n",
    "                                                        max_episodes = 20000,\n",
    "                                                        n=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MIE1624_env",
   "language": "python",
   "name": "mie1624_env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
